{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldsFloat = [\"Spend\", \"Revenue\", \"Budget\", \"ROAS\", \"CPI\", \"CPP\", \"CohortNum\", \"DayNum\", \"Reach\", \"Impressions\", \"Clicks\", \"Installs\", \"Purchases\", \"Payers\", \"Opt_goal_conversion_window\", \"Age Min\", \"Age Max\"]\n",
    "\n",
    "game = \"Game_11-55\"#\"Game_3c-e1\"\n",
    "#games = [Game_f0-90, Game_5Ñ-16, Game_11-55, Game_0d-82,\n",
    "#         Game_14-2e, Game_38-2a, Game_a4-99, Game_a2-fc,\n",
    "#         Game_bd-be, Game_68-1e, Game_b5-f6, Game_3c-e1,\n",
    "#         Game_50-58, Game_d9-df, Game_da-e4, Game_a7-b9\n",
    "# mid: [1, 11, 13]\n",
    "# large: [2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15]\n",
    "\n",
    "featDays = [2]\n",
    "targDays = [2]\n",
    "z_dims = [4]\n",
    "train_batch_size = 32\n",
    "N = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../common')\n",
    "import DataGetter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datagetter = DataGetter.DataGetter()\n",
    "datagetter.setDataPath(\"../data2\")\n",
    "games = datagetter.getGamesList()\n",
    "\n",
    "def dataLoadDays(featDay, targDay):\n",
    "    # features day and target day\n",
    "    loadgames = {}\n",
    "    for game in games:\n",
    "        (i_revenue_all, i_revenue_not_null, df) = datagetter.getGamesData([game], [], [])\n",
    "        validInds = pd.Series([False] * len(df))\n",
    "        for adset in df[\"Adset\"].unique():\n",
    "            for cohort in df[df[\"Adset\"] == adset][\"CohortNum\"].unique():\n",
    "                ind = (df[\"Adset\"] == adset) & (df[\"CohortNum\"] == cohort)\n",
    "                dft = df[ind]\n",
    "                if len(dft[dft[\"DayNum\"]==targDay]) > 0:\n",
    "                    validInds = (validInds) | (ind)\n",
    "        loadgames[game] = df[validInds]\n",
    "        \n",
    "    return loadgames\n",
    "\n",
    "\n",
    "def dataLoadGameDays(game, featDay, targDay):\n",
    "    # features day and target day\n",
    "    (i_revenue_all, i_revenue_not_null, df) = datagetter.getGamesData([game], [], [])\n",
    "    #validInds = pd.Series([False] * len(df))\n",
    "    #for adset in df[\"Adset\"].unique():\n",
    "    #    for cohort in df[df[\"Adset\"] == adset][\"CohortNum\"].unique():\n",
    "    #        ind = (df[\"Adset\"] == adset) & (df[\"CohortNum\"] == cohort)\n",
    "    #        dft = df[ind]\n",
    "    #        if len(dft[dft[\"DayNum\"]==targDay]) > 0:\n",
    "    #            validInds = (validInds) | (ind)\n",
    "    #    \n",
    "    #return df[validInds]\n",
    "    dfAcT = df[df[\"DayNum\"]==targDays[0]][[\"Adset\", \"CohortNum\"]].drop_duplicates()\n",
    "    dfAcA = df[[\"Adset\", \"CohortNum\"]]\n",
    "    dfAcA[\"common\"] = dfAcA[\"Adset\"] + dfAcA[\"CohortNum\"].astype(str)\n",
    "    dfAcT[\"common\"] = dfAcT[\"Adset\"] + dfAcT[\"CohortNum\"].astype(str)\n",
    "    dfAcAV = dfAcA[\"common\"].values\n",
    "    dfAcTV = dfAcT[\"common\"].values\n",
    "    #print(len(dfAcAV), len(dfAcTV))\n",
    "    ind = [x in dfAcTV for x in dfAcAV]\n",
    "    #print(len(ind), sum(ind))\n",
    "    return df[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all cohorts: 27561 \n",
      "  number of cohorts with revenues = zero: 22822 \n",
      "  number of cohorts with revenues != zero: 4739\n"
     ]
    }
   ],
   "source": [
    "(i_revenue_all, i_revenue_not_null, df) = datagetter.getGamesData([game], [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = dataLoadGameDays(game, featDays[0], targDays[0])\n",
    "#df = df[df[\"DayNum\"]==featDays[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../common')\n",
    "import DataGetter\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def getDfWithFeatures2(game, targDay, advancedFeatures=False):\n",
    "    datagetter = DataGetter.DataGetter()\n",
    "    datagetter.setDataPath(\"../data2\")\n",
    "    (i_revenue_all, i_revenue_not_null, df) = datagetter.getGamesData([game], [], [])\n",
    "\n",
    "    dfAcT = df[df[\"DayNum\"]==targDays[0]][[\"Adset\", \"CohortNum\"]].drop_duplicates()\n",
    "    dfAcA = df[[\"Adset\", \"CohortNum\"]]\n",
    "    dfAcA[\"common\"] = dfAcA[\"Adset\"] + dfAcA[\"CohortNum\"].astype(str)\n",
    "    dfAcT[\"common\"] = dfAcT[\"Adset\"] + dfAcT[\"CohortNum\"].astype(str)\n",
    "    dfAcAV = dfAcA[\"common\"].values\n",
    "    dfAcTV = dfAcT[\"common\"].values\n",
    "    #print(len(dfAcAV), len(dfAcTV))\n",
    "    ind = [x in dfAcTV for x in dfAcAV]\n",
    "    #print(len(ind), sum(ind))\n",
    "\n",
    "    cols = [c for c in df.columns if c not in [\"Adset\", \"DateCohort\", \"DateDay\", \"Date\", \"CampId\", \"AccId\"]]\n",
    "    df = df[ind][cols]\n",
    "\n",
    "    df = df.replace([math.inf, -math.inf], math.nan)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_full_data(game, featDay=2, targDay=13, targFeature=\"Payers\", train_batch_size=32):\n",
    "    print('loading data!')\n",
    "    df = getDfWithFeatures2(game, targDay)\n",
    "    \n",
    "    # features types processing\n",
    "    columns = df.columns\n",
    "    columnsToDel = []\n",
    "    for c in columns:\n",
    "        if len(np.unique(df[c])) == 1:\n",
    "            columnsToDel.append(c)\n",
    "    df = df[[c for c in columns if c not in columnsToDel]]\n",
    "    columns = df.columns\n",
    "    #\n",
    "    columnsToDel = []\n",
    "    for c in columns:\n",
    "        #try:\n",
    "        #    np.unique(df[c])\n",
    "        #except:\n",
    "        #    print(c)\n",
    "        #    print(df[c])\n",
    "        if type(df.iloc[0][c]) == str:\n",
    "            df[c] = df[c].astype(str)\n",
    "            if len(np.unique(df[c])) == 2:\n",
    "                le = LabelEncoder()\n",
    "                le.fit(df[c])\n",
    "                df[c] = le.transform(df[c])\n",
    "            else:\n",
    "                enc = OneHotEncoder(sparse=False)\n",
    "                new_ohe_features = enc.fit_transform(df[c].values.reshape(-1, 1))\n",
    "                tmp = pd.DataFrame(new_ohe_features, columns=[c+\"_\"+str(i) for i in range(new_ohe_features.shape[1])])\n",
    "                df = pd.concat([df, tmp], axis=1)\n",
    "                columnsToDel.append(c)\n",
    "    columns = df.columns\n",
    "    df = df[[c for c in columns if c not in columnsToDel]]\n",
    "    #print(df)\n",
    "    columns = df.columns\n",
    "    #print(df)\n",
    "    print(len(df.columns))\n",
    "    \n",
    "    # fill load list\n",
    "    feat_keys = [c for c in df.columns if c != \"DayNum\" and c != targFeature]\n",
    "    \n",
    "    for f in df.columns:\n",
    "        if type(df.iloc[0][f]) == str or type(df.iloc[0][f]) == object:\n",
    "            df[f] = df[f].fillna(\"\")\n",
    "            df[f] = np.where(pd.isnull(df[f].values), \"\", df[f].values)\n",
    "        else:\n",
    "            df[f] = df[f].fillna(0)\n",
    "            df[f] = np.where(pd.isnull(df[f].values), 0, df[f].values)\n",
    "\n",
    "    #df['norm'] = np.where(df['Impressions'] <= 0, 1, df['Impressions'])\n",
    "    #print(df['norm'])\n",
    "    #for c in df.columns:\n",
    "    #    if c in fieldsFloat and c != \"norm\":\n",
    "    #        df[c] = df[c] / df[\"norm\"]\n",
    "    #df = df[[c for c in columns if c != \"norm\" and c != \"Impressions\"]]\n",
    "    #feat_keys = [f for f in feat_keys if f != \"Impressions\"]\n",
    "    \n",
    "    nparr = df[feat_keys + [targFeature]].values\n",
    "    maxs = np.max(nparr, axis = 0)\n",
    "    load_list = []\n",
    "    \n",
    "    #print(df.columns, maxs, df)\n",
    "    \n",
    "    #falseCols = []\n",
    "    #for c in df.columns:\n",
    "    #    if np.sum(pd.isnull(df[c])):\n",
    "    #        print(c)\n",
    "    #        falseCols.append(c)\n",
    "    #print(maxs)\n",
    "    #print(df[falseCols])\n",
    "\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i]['DayNum'] == featDay:\n",
    "            lst = df.iloc[i][feat_keys].tolist()\n",
    "            lst.append(df.iloc[i + targDay - featDay][targFeature])\n",
    "            i = i + targDay - featDay\n",
    "            # print(lst)\n",
    "            load_list.append(lst)\n",
    "\n",
    "    # normalize list by maxs\n",
    "    i = 0\n",
    "    for lst in load_list:\n",
    "        lst_ = np.array(lst)\n",
    "        lst_ = lst_ / maxs\n",
    "        load_list[i] = lst_.tolist()\n",
    "        #print(lst_, maxs, load_list[i])\n",
    "        i = i + 1\n",
    "\n",
    "    # fill array with train_batch_size\n",
    "    if train_batch_size>0:\n",
    "        data = np.zeros(shape=(int(len(load_list) / train_batch_size), train_batch_size, len(feat_keys) + 1), dtype=float)\n",
    "        dt = np.zeros(shape=(train_batch_size, len(feat_keys) + 1), dtype=float)\n",
    "        upindex = 0\n",
    "        index = 0\n",
    "        for lst in load_list:\n",
    "            dt[index, :] = lst\n",
    "            index = index + 1\n",
    "            if index == train_batch_size:\n",
    "                index = 0\n",
    "                #print(dt)\n",
    "                data[upindex, :, :] = dt\n",
    "                # if upindex==0:\n",
    "                #    print(dt)\n",
    "                dt = np.zeros(shape=(train_batch_size, len(feat_keys) + 1), dtype=float)\n",
    "                upindex = upindex + 1\n",
    "\n",
    "        data = torch.from_numpy(data)  # learn_list)\n",
    "        # X = Variable(torch.from_numpy(learn_list).float(), requires_grad = True)\n",
    "    else:\n",
    "        data = torch.from_numpy(np.array(load_list))\n",
    "\n",
    "    return (data, maxs, feat_keys+[targFeature])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data!\n",
      "Number of all cohorts: 27561 \n",
      "  number of cohorts with revenues = zero: 22822 \n",
      "  number of cohorts with revenues != zero: 4739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdeev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#import sys\n",
    "#sys.path.insert(0, '../common')\n",
    "#from DataLoaderForEncoder import load_data\n",
    "\n",
    "(data, maxs, cols) = load_full_data(game, featDays[0], targDays[0], \"Payers\", 32)\n",
    "#cols = [c for c in cols if c!= \"Impressions\"]\n",
    "#with open('./pickles/maxs_' + games[0] + '_%d_%d__.pickle' % (featDays[0], targDays[0]), 'wb') as f:\n",
    "#                    pickle.dump(maxs, f)\n",
    "with open('maxs_' + game + '_%d_%d__.pickle' % (featDays[0], targDays[0]), 'wb') as f:\n",
    "                    pickle.dump(maxs, f)\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.61000e+02, 1.00000e+00, 9.21440e+02, 9.95810e+02, 1.11485e+03,\n",
       "       1.39191e+05, 1.57936e+05, 2.33200e+03, 7.65000e+02, 9.50000e+01,\n",
       "       6.70340e+02, 4.23100e+01, 2.73030e+02, 1.00000e+00, 7.00000e+00,\n",
       "       2.50000e+01, 6.50000e+01, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "       1.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 4.70000e+01])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0; loss: 8.987e+05\n",
      "Epoch-10; loss: 3.795e+05\n",
      "Epoch-20; loss: 3.608e+05\n",
      "Epoch-30; loss: 3.529e+05\n",
      "Epoch-40; loss: 3.465e+05\n",
      "Epoch-50; loss: 3.429e+05\n",
      "Epoch-60; loss: 3.402e+05\n",
      "Epoch-70; loss: 3.376e+05\n",
      "Epoch-80; loss: 3.353e+05\n",
      "Epoch-90; loss: 3.33e+05\n",
      "Epoch-100; loss: 3.315e+05\n",
      "Epoch-110; loss: 3.303e+05\n",
      "Epoch-120; loss: 3.29e+05\n",
      "Epoch-130; loss: 3.279e+05\n",
      "Epoch-140; loss: 3.265e+05\n",
      "Epoch-150; loss: 3.257e+05\n",
      "Epoch-160; loss: 3.252e+05\n",
      "Epoch-170; loss: 3.242e+05\n",
      "Epoch-180; loss: 3.235e+05\n",
      "Epoch-190; loss: 3.236e+05\n",
      "Epoch-200; loss: 3.219e+05\n",
      "Epoch-210; loss: 3.225e+05\n",
      "Epoch-220; loss: 3.217e+05\n",
      "Epoch-230; loss: 3.22e+05\n",
      "Epoch-240; loss: 3.205e+05\n",
      "Epoch-250; loss: 3.202e+05\n",
      "Epoch-260; loss: 3.196e+05\n",
      "Epoch-270; loss: 3.187e+05\n",
      "Epoch-280; loss: 3.188e+05\n",
      "Epoch-290; loss: 3.191e+05\n",
      "Epoch-300; loss: 3.19e+05\n",
      "Epoch-310; loss: 3.177e+05\n",
      "Epoch-320; loss: 3.188e+05\n",
      "Epoch-330; loss: 3.164e+05\n",
      "Epoch-340; loss: 3.171e+05\n",
      "Epoch-350; loss: 3.168e+05\n",
      "Epoch-360; loss: 3.181e+05\n",
      "Epoch-370; loss: 3.171e+05\n",
      "Epoch-380; loss: 3.163e+05\n",
      "Epoch-390; loss: 3.162e+05\n",
      "Epoch-400; loss: 3.162e+05\n",
      "Epoch-410; loss: 3.155e+05\n",
      "Epoch-420; loss: 3.154e+05\n",
      "Epoch-430; loss: 3.15e+05\n",
      "Epoch-440; loss: 3.158e+05\n",
      "Epoch-450; loss: 3.154e+05\n",
      "Epoch-460; loss: 3.144e+05\n",
      "Epoch-470; loss: 3.148e+05\n",
      "Epoch-480; loss: 3.141e+05\n",
      "Epoch-490; loss: 3.135e+05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../encoder1')\n",
    "from VaeEncoder import VaeEncoder, VAE2\n",
    "\n",
    "#print(data.shape[1])\n",
    "vae = VaeEncoder(X_dim=data.shape[2], N=32, z_dim=z_dims[0])\n",
    "(model, losses) = vae.generate_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../common')\n",
    "import DataGetter\n",
    "import pandas as pd\n",
    "\n",
    "datagetter = DataGetter.DataGetter()\n",
    "datagetter.setDataPath(\"../data\")\n",
    "games = datagetter.getGamesList()\n",
    "\n",
    "#columns = [\"Spend\", \"Reach\", \"Impressions\", \"Clicks\", \"Installs\", \"Payers\"]\n",
    "\n",
    "maxDensityCells = 1600\n",
    "samplesCount = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from DataLoaderForEncoder import load_data\n",
    "\n",
    "def loadGameDataForDensity(game, featDay, targDay):\n",
    "    (data, norms, cols) = load_full_data(game, featDay, targDay, train_batch_size=0)\n",
    "\n",
    "    print(type(data), data.shape)\n",
    "    data = data.float()\n",
    "    data = torch.autograd.Variable(data)\n",
    "    if torch.cuda.is_available():\n",
    "        data = data.cuda()\n",
    "\n",
    "    return (data, norms, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import sys\n",
    "sys.path.insert(0, '../encoder1')\n",
    "#import AaeEncoder\n",
    "\n",
    "#print(__doc__)\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def getHiddenGameLayerForDensity(game, featDay=2, targDay=13, hiddenNeurons=2):\n",
    "    (data, norms, cols) = loadGameDataForDensity(game, featDay, targDay)\n",
    "    #\"VAE__;seed_10;z_4;N_32;batch_32;opt_Adam;dropout=0.100000.pickle\"\n",
    "    #qName = 'VAE_' + game + ('_%d_%d_' % (featDay, targDay)) + \\\n",
    "    qName = 'VAE_' + \\\n",
    "            ('_;seed_10;z_%d;N_%d;batch_%d;opt_Adam;dropout=0.100000.pickle' % (z_dims[0], N, train_batch_size))\n",
    "    #qName = \"VAE__;seed_10;z_2;N_32;batch_32;opt_Adam;dropout=0.100000.pickle\"\n",
    "    model = torch.load(qName)\n",
    "    #result = model(data) # ret of forward\n",
    "    result = model.encode(data) # mu, logvar\n",
    "    #if torch.cuda.is_available():\n",
    "    #    result = result.cpu()\n",
    "\n",
    "    res = []\n",
    "    if torch.cuda.is_available():\n",
    "        res.append(result[0].cpu().data.numpy())\n",
    "        res.append(result[1].cpu().data.numpy())\n",
    "    else:\n",
    "        res.append(result[0].data.numpy())\n",
    "        res.append(result[1].data.numpy())\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data!\n",
      "Number of all cohorts: 27561 \n",
      "  number of cohorts with revenues = zero: 22822 \n",
      "  number of cohorts with revenues != zero: 4739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdeev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "<class 'torch.Tensor'> torch.Size([27487, 210])\n"
     ]
    }
   ],
   "source": [
    "result = getHiddenGameLayerForDensity(game, featDays[0], targDays[0])\n",
    "#result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS of hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X=result[0].tolist()\n",
    "mds=manifold.MDS(n_components=2)\n",
    "X_r=mds.fit_transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 8))\n",
    "ax.scatter([x[0] for x in X_r], [x[1] for x in X_r])\n",
    "plt.show()\n",
    "\n",
    "print(len(X_r[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances between samples in hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rs = []\n",
    "#X = result[0].tolist()\n",
    "n = len(X)\n",
    "m = len(X[0])\n",
    "for i in range(n - 1):\n",
    "    for j in range(i + 1, n):\n",
    "        x = X[i]\n",
    "        y = X[j]\n",
    "        rs.append(sum([(x[k]-y[k])*(x[k]-y[k]) for k in range(m)]))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 5))\n",
    "ax.hist(rs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of columns for hidden layer: dR_{hidden} / dCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "(data, norms, cols) = loadGameDataForDensity(game, featDays[0], targDays[0])\n",
    "datan  = data.data.numpy()\n",
    "#normsn = norms\n",
    "\n",
    "deltasData = np.zeros([datan.shape[1] + 1, datan.shape[1]]) # len(cols)+1, len(cols)\n",
    "means = np.mean(datan, axis=0)\n",
    "maxs  = np.max(datan, axis=0)\n",
    "mins  = np.min(datan, axis=0)\n",
    "dels  = maxs - mins\n",
    "deltasData[0] = means\n",
    "for i in range(datan.shape[1]):\n",
    "    delta = [0] * datan.shape[1]\n",
    "    delta[i] = dels[i] / 100 #0.01\n",
    "    deltasData[1 + i] = means + np.array(delta)\n",
    "deltasData = torch.from_numpy(deltasData)\n",
    "deltasData = deltasData.float()\n",
    "deltasData = torch.autograd.Variable(deltasData)\n",
    "if torch.cuda.is_available():\n",
    "    deltasData = deltasData.cuda()\n",
    "\n",
    "qName = 'VAE_' + \\\n",
    "        ('_;seed_10;z_%d;N_%d;batch_%d;opt_Adam;dropout=0.100000.pickle' % (z_dims[0], N, train_batch_size))\n",
    "\n",
    "model = torch.load(qName)\n",
    "\n",
    "result = model.encode(deltasData) # mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    Xnp = result[0].cpu().data.numpy()\n",
    "else:\n",
    "    Xnp = result[0].data.numpy()\n",
    "\n",
    "weights = []\n",
    "for i in range(len(Xnp) - 1):\n",
    "    weights.append( sum((Xnp[i+1]-Xnp[0]) * (Xnp[i+1]-Xnp[0]) ) )\n",
    "    \n",
    "weightsSer = pd.Series(weights, cols)\n",
    "weightsSer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations of columns with Payers of hidden layer\n",
    "We have vector in hidden layer with dPayers and vectors in hidden layer with dColumns\n",
    "Correlation is scalar productions of these vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rHidPayers = Xnp[-1] - Xnp[0]\n",
    "rHidCols = []\n",
    "for i in range(len(Xnp) - 2):\n",
    "    rHidCols.append(Xnp[i+1] - Xnp[0])\n",
    "\n",
    "corrs = [ sum(rHidPayers * rHidCol) * 10000 for rHidCol in rHidCols]\n",
    "corrsSer = pd.Series(corrs, cols[:-1])\n",
    "corrsSer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Payers with generation from center point in hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "mu = X[0]\n",
    "logvar = result[1][0] # logvar.exp() = sigma2\n",
    "if torch.cuda.is_available():\n",
    "    logvar = logvar.cpu().data.numpy()\n",
    "else:\n",
    "    logvar = logvar.data.numpy()\n",
    "sigma = np.sqrt(np.exp(logvar))\n",
    "\n",
    "def f(x):\n",
    "    hidPoint = np.random.normal(mu, sigma)\n",
    "    hidPoint = Variable(torch.from_numpy(np.array(hidPoint)).float())\n",
    "    gen = model.decode(hidPoint)\n",
    "    if torch.cuda.is_available():\n",
    "        return gen.cpu().data.numpy() * norms\n",
    "    else:\n",
    "        return gen.data.numpy() * norms\n",
    "     \n",
    "g = np.vectorize(f, otypes=[np.ndarray])\n",
    "a = np.arange(10000)\n",
    "generated = g(a)\n",
    "#print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist([g[-1] for g in generated])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters in hidden layer research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def dbscan(points):\n",
    "    dbscan = cluster.DBSCAN(eps=0.3, min_samples=3)\n",
    "    dbscan.fit(points)\n",
    "    pointLabels = dbscan.labels_\n",
    "    core = dbscan.core_sample_indices_\n",
    "    #print( repr(core) )\n",
    "    size = [5 if i not in core else 40 for i in range(len(X))]\n",
    "    #print( repr(size) )\n",
    "    return pointLabels\n",
    "\n",
    "pointLabels = dbscan(X)\n",
    "\n",
    "mds=manifold.MDS(n_components=2)\n",
    "X_r=mds.fit_transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 8))\n",
    "ax.scatter([x[0] for x in X_r], [x[1] for x in X_r], \n",
    "            c=['bgrcmyk'[pointLabels[i] % 7] for i in range(len(X_r))],\n",
    "            alpha=0.8, marker='o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AffinityPropagation clustering (for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def affclust(points):\n",
    "    clustering = AffinityPropagation(preference=-45).fit(points)\n",
    "    return clustering.labels_\n",
    "\n",
    "pointLabels = affclust(X)\n",
    "\n",
    "mds=manifold.MDS(n_components=2)\n",
    "X_r=mds.fit_transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 8))\n",
    "ax.scatter([x[0] for x in X_r], [x[1] for x in X_r], \n",
    "            c=['bgrcmyk'[pointLabels[i] % 7] for i in range(len(X_r))],\n",
    "            alpha=0.8, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean features values for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointLabels = dbscan(X)\n",
    "#print(np.unique(pointLabels))\n",
    "\n",
    "(data, norms, cols) = load_full_data(game, featDays[0], targDays[0], train_batch_size=0)\n",
    "data = data.data.numpy()\n",
    "data = data * norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=([\"N points\"]+cols), columns=[str(cl+1) for cl in range(max(pointLabels))])\n",
    "for cluster in range(max(pointLabels) + 1):\n",
    "    ind = (np.array(pointLabels) == cluster)\n",
    "    Npoints = sum(ind)\n",
    "    dt = data[ind]\n",
    "    #print(Npoints, ind, len(dt))\n",
    "    #print(np.mean(dt, axis=0))\n",
    "    df.loc[\"N points\"][\"%d\" % (cluster+1)] = Npoints\n",
    "    means = np.mean(dt, axis=0)\n",
    "    for i in range(len(cols)):\n",
    "        df.loc[cols[i]][\"%d\" % (cluster+1)] = means[i]\n",
    "    #print(df)\n",
    "    #break\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bts = b\"12;34;1\\n234\"\n",
    "bts.index(b\"\\n\")\n",
    "bts[:bts.index(b\"\\n\")].decode(\"utf-8\").split(\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
